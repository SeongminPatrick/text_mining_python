{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이트 경제 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "주어진 날짜의 네이트 경제 뉴스를 수집하여 저장한다.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import ujson\n",
    "\n",
    "\n",
    "LIST_PAGE_URL_TMPL = \"http://news.nate.com/recent?cate=eco&mid=n0301&type=c&date={}&page={}\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \" + \\\n",
    "             \"AppleWebKit/537.36 (KHTML, like Gecko) \" + \\\n",
    "             \"Chrome/37.0.2062.94 Safari/537.36\"\n",
    "HEADERS = {\"User-Agent\": USER_AGENT}\n",
    "PAGING_STOP_PAT = \"뉴스가 없습니다.\"\n",
    "SLEEP_TIME = 2\n",
    "\n",
    "\n",
    "def get_target_date():\n",
    "    \"\"\"경제 뉴스 수집 대상 날짜를 입력받아 돌려준다.\"\"\"\n",
    "    \n",
    "    target_date = input(\"네이트 경제 뉴스 수집 대상 날짜를 입력하세요(YYYYMMDD): \")\n",
    "    \n",
    "    return target_date\n",
    "   \n",
    "    \n",
    "def crawl_articles(target_date):\n",
    "    \"\"\"주어진 수집 대상 날짜의 네이트 경제 뉴스 기사를 수집하여 저장한다.\"\"\"\n",
    "    \n",
    "    output_file_name = gen_output_file_name(target_date)\n",
    "    \n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:   \n",
    "        page_num = 1\n",
    "        while True:\n",
    "            article_urls = fetch_article_urls(target_date, page_num)\n",
    "            \n",
    "            if not article_urls:\n",
    "                break\n",
    "                \n",
    "            fetch_store_articles(output_file, article_urls)\n",
    "            page_num += 1\n",
    "        \n",
    "        \n",
    "def gen_output_file_name(target_date):\n",
    "    \"\"\"주어진 날짜를 이용하여 출력 파일 이름을 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    output_file_name = \"../data/crawling/nate-economy-articles-{}.txt\".format(target_date)\n",
    "    \n",
    "    return output_file_name\n",
    "        \n",
    "        \n",
    "def fetch_article_urls(target_date, page_num):\n",
    "    \"\"\"주어진 날짜와 페이지 번호의 목록 페이지에서 기사 URL들을 수집하여 돌려준다.\"\"\"\n",
    "\n",
    "    list_page_url = gen_list_page_url(target_date, page_num)\n",
    "    html = get_html(list_page_url)\n",
    "\n",
    "    if paging_done(html):\n",
    "        return None\n",
    "\n",
    "    soup = get_soup(html)\n",
    "    article_urls = ext_article_urls(soup)\n",
    "    \n",
    "    return article_urls\n",
    "\n",
    "\n",
    "def gen_list_page_url(target_date, page_num):\n",
    "    \"\"\"기사 목록 페이지 URL을 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    list_page_url = LIST_PAGE_URL_TMPL.format(target_date, page_num)\n",
    "    \n",
    "    return list_page_url\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"주어진 URL에 접근하여 HTML 텍스트를 읽어서 돌려준다.\"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    html = response.text\n",
    "    pause()\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def pause():\n",
    "    \"\"\"정해진 시간만큼 쉰다.\"\"\"\n",
    "\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    \n",
    "    \n",
    "def get_soup(html):\n",
    "    \"\"\"주어진 HTML 텍스트를 soup 객체로 만들어 돌려준다.\"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "def paging_done(html):\n",
    "    \"\"\"페이징이 완료되었는지를 판단한다.\"\"\"\n",
    "\n",
    "    if PAGING_STOP_PAT in html:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def ext_article_urls(soup):\n",
    "    \"\"\"주어진 soup 객체에서 기사 URL을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    anchor_elems = soup.find_all(\"a\", class_=\"lt1\")\n",
    "    article_urls = [elem[\"href\"] for elem in anchor_elems]\n",
    "\n",
    "    return article_urls\n",
    "\n",
    "\n",
    "def fetch_store_articles(output_file, article_urls):\n",
    "    \"\"\"주어진 기사 URL의 기사를 수집하여 출력 파일에 저장한다.\"\"\"\n",
    "    \n",
    "    for article_url in article_urls:\n",
    "        html = get_html(article_url)\n",
    "        soup = get_soup(html)\n",
    "        title = ext_title(soup)\n",
    "        date_time = ext_date_time(soup)\n",
    "        body = ext_body(soup)\n",
    "        write_article(output_file, title, date_time, body)   \n",
    "\n",
    "    \n",
    "def ext_title(soup):\n",
    "    \"\"\"주어진 soup 객체에서 기사 제목을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    title_elem = soup.find(\"h3\", class_=\"articleSubecjt\")\n",
    "    title = title_elem.string\n",
    "    \n",
    "    return title\n",
    "\n",
    "            \n",
    "def ext_date_time(soup):\n",
    "    \"\"\"주어진 soup 객체에서 기사 날짜, 시간을 추출하여 돌려준다.\"\"\"\n",
    "   \n",
    "    outer_elem = soup.find(\"span\", class_=\"firstDate\")\n",
    "    date_time_elem = outer_elem.find(\"em\")\n",
    "    date_time = date_time_elem.string\n",
    "    \n",
    "    return date_time\n",
    "    \n",
    "    \n",
    "def ext_body(soup):\n",
    "    \"\"\"주어진 soup 객체에서 기사 본문을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    outer_elem = soup.find(\"div\", id=\"realArtcContents\")\n",
    "    body = outer_elem.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    return body\n",
    "       \n",
    "\n",
    "def write_article(output_file, title, date_time, body):\n",
    "    \"\"\"기사를 JSON 문자열로 만들어 출력 파일에 기록한다.\"\"\"\n",
    "    \n",
    "    article = {\"title\": title, \"date_time\": date_time, \"body\": body}\n",
    "    json_str = ujson.dumps(article, ensure_ascii=False)\n",
    "    print(json_str, file=output_file, flush=True)\n",
    "    print(json_str, flush=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"주어진 수집 대상 날짜의 네이트 경제 뉴스 기사를 수집하여 저장한다.\"\"\"\n",
    "    \n",
    "    target_date = get_target_date()\n",
    "    crawl_articles(target_date)\n",
    "    \n",
    "#\n",
    "# 실행\n",
    "#\n",
    "    \n",
    "main()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
