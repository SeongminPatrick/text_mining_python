{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인벤 게시판 댓글 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "인벤 오픈 이슈 갤러리 게시글의 댓글을 수집한다.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import ujson\n",
    "\n",
    "\n",
    "LIST_PAGE_URL_TMPL = \"http://www.inven.co.kr/board/powerbbs.php?come_idx=2097&query=list&my=&category=&category2=&sort=PID&orderby=&name=&subject=&content=&keyword=&sterm=&eq=&iskin=webzine&mskin=&p={}\"\n",
    "COMMENT_REQ_URL = \"http://www.inven.co.kr/common/board/comment.xml.php?dummy=1502246098813\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \" + \\\n",
    "             \"AppleWebKit/537.36 (KHTML, like Gecko) \" + \\\n",
    "             \"Chrome/37.0.2062.94 Safari/537.36\"\n",
    "HEADERS = {\"User-Agent\": USER_AGENT}\n",
    "PAGING_STOP_PAT = \"현재 등록된 게시물이 없습니다.\"\n",
    "SLEEP_TIME = 2\n",
    "\n",
    "\n",
    "def crawl_comments(output_file_name):\n",
    "    \"\"\"인벤 오픈 이슈 갤러리 게시글의 댓글을 수집하여 주어진 이름의 파일에 저장한다.\"\"\"\n",
    "    \n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:   \n",
    "        page_num = 1\n",
    "        while True:\n",
    "            post_ids = fetch_post_ids(page_num)\n",
    "            \n",
    "            if post_ids is None:\n",
    "                break\n",
    "                \n",
    "            fetch_store_comments(output_file, post_ids)\n",
    "            page_num += 1\n",
    "            break\n",
    "        \n",
    "        \n",
    "def fetch_post_ids(page_num):\n",
    "    \"\"\"주어진 페이지 번호의 목록 페이지에서 댓글이 있는 게시물 ID들을 수집하여 돌려준다.\"\"\"\n",
    "\n",
    "    list_page_url = gen_list_page_url(page_num)\n",
    "    html = get_html(list_page_url)\n",
    "\n",
    "    if paging_done(html):\n",
    "        return None\n",
    "\n",
    "    soup = get_soup(html)\n",
    "    post_ids = ext_post_ids(soup)\n",
    "    \n",
    "    return post_ids\n",
    "\n",
    "\n",
    "def gen_list_page_url(page_num):\n",
    "    \"\"\"게시글 목록 페이지 URL을 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    list_page_url = LIST_PAGE_URL_TMPL.format(page_num)\n",
    "    \n",
    "    return list_page_url\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"주어진 URL에 접근하여 HTML 텍스트를 읽어서 돌려준다.\"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    html = response.text\n",
    "    pause()\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def pause():\n",
    "    \"\"\"정해진 시간만큼 쉰다.\"\"\"\n",
    "\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    \n",
    "    \n",
    "def get_soup(html):\n",
    "    \"\"\"주어진 HTML 텍스트를 soup 객체로 만들어 돌려준다.\"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "def paging_done(html):\n",
    "    \"\"\"페이징이 완료되었는지를 판단한다.\"\"\"\n",
    "\n",
    "    if PAGING_STOP_PAT in html:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def ext_post_ids(soup):\n",
    "    \"\"\"주어진 soup 객체에서 댓글이 있는 게시물 ID들을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    post_id_elems = soup.find_all(\"span\", class_=\"sj_cm\")\n",
    "    post_ids = [elem[\"data-cmt-uid\"] for elem in post_id_elems]\n",
    "\n",
    "    return post_ids\n",
    "\n",
    "\n",
    "def fetch_store_comments(output_file, post_ids):\n",
    "    \"\"\"주어진 게시글 ID의 댓글들을 수집하여 출력 파일에 저장한다.\"\"\"\n",
    "    \n",
    "    for post_id in post_ids:\n",
    "        xml = get_comment_xml(post_id)\n",
    "        soup = get_soup(xml)\n",
    "        authors = ext_authors(soup)\n",
    "        date_times = ext_date_times(soup)\n",
    "        bodies = ext_bodies(soup)\n",
    "        write_comments(output_file, post_id, authors, date_times, bodies)   \n",
    "        \n",
    "        \n",
    "def get_comment_xml(post_id):\n",
    "    \"\"\"주어진 게시글 ID에 대한 댓글 XML 텍스트를 요청하여 돌려준다.\"\"\"\n",
    "    \n",
    "    req_data = gen_comment_req_data(post_id)\n",
    "    resp = requests.post(COMMENTS_REQ_URL, data=req_data)\n",
    "    xml = resp.text\n",
    "    pause()\n",
    "    \n",
    "    return xml\n",
    "\n",
    "\n",
    "def gen_comment_req_data(post_id):\n",
    "    \"\"\"주어진 게시글 ID의 댓글 페이지 요청 데이터를 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    req_data = {\n",
    "        \"comeidx\": 2097,\n",
    "        \"articlecode\": post_id,\n",
    "        \"sortorder\": \"date\",\n",
    "        \"uploadurl\": \"\",\n",
    "        \"imageposition\": \"\",\n",
    "        \"act\": \"list\",\n",
    "        \"out\": \"xml\",\n",
    "        \"replynick\": \"\",\n",
    "        \"replyidx\": 0   \n",
    "    }     \n",
    "\n",
    "    return req_data\n",
    "        \n",
    "    \n",
    "def ext_authors(soup):\n",
    "    \"\"\"주어진 soup 객체에서 게시자들을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    author_elems = soup.find_all(\"o_name\")\n",
    "    authors = [elem.string for elem in author_elems]\n",
    "    \n",
    "    return authors\n",
    "\n",
    "            \n",
    "def ext_date_times(soup):\n",
    "    \"\"\"주어진 soup 객체에서 게시 날짜, 시간을 추출하여 돌려준다.\"\"\"\n",
    "   \n",
    "    date_time_elems = soup.find_all(\"o_date\")\n",
    "    date_times = [elem.string for elem in date_time_elems]\n",
    "    \n",
    "    return date_times\n",
    "    \n",
    "    \n",
    "def ext_bodies(soup):\n",
    "    \"\"\"주어진 soup 객체에서 댓글 본문을 추출하여 돌려준다.\"\"\"\n",
    "    \n",
    "    body_elems = soup.find_all(\"o_comment\")\n",
    "    bodies = [elem.string.replace(\"&nbsp;\", \" \") for elem in body_elems]\n",
    "\n",
    "    return bodies\n",
    "       \n",
    "\n",
    "def write_comments(output_file, post_id, authors, date_times, bodies):\n",
    "    \"\"\"댓글을 JSON 문자열로 만들어 출력 파일에 기록한다.\"\"\"\n",
    "                   \n",
    "    for author, date_time, body in zip(authors, date_times, bodies):\n",
    "        comment = {\"post_id\": post_id, \"author\": author, \"date_time\": date_time, \n",
    "                   \"body\": body}\n",
    "        json_str = ujson.dumps(comment, ensure_ascii=False)\n",
    "        print(json_str, file=output_file)\n",
    "              \n",
    "    output_file.flush()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"인벤 오픈 이슈 갤러리 게시글의 댓글을 수집하여 저장한다.\"\"\"\n",
    "    \n",
    "    output_file_name = \"../data/crawling/inven-open-issue-comments.txt\"\n",
    "    crawl_comments(output_file_name)\n",
    "    \n",
    "#\n",
    "# 실행\n",
    "#\n",
    "    \n",
    "main()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
