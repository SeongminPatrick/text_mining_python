{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "* 형태소 분석의 필요성과 개괄적인 원리를 이해한다.\n",
    "* konlpy 라이브러릴 이용한 형태소 분석 방법을 익힌다.\n",
    "* JSON 라인 형식의 형태소 분석 파일의 작성 기법을 실습을 통해 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 설치\n",
    "### 개요\n",
    "이번 강좌에서 새로이 사용하는 라이브러리는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 명칭 | 기능 | 배포판 포함 여부 | 설치 도구 | 홈페이지 |\n",
    "|:-----|:-----|:--------------:|:---------:|:-----------|\n",
    "| konlpy | 한국어 형태소 분석기 통합 인터페이스 | 미포함 | pip | <http://konlpy.org>\n",
    "| jpype1 | 자바 라이브러리와 파이썬의 연결 | 미포함 | conda | <http://jpype.readthedocs.io/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konlpy 라이브러리에서 자바로 구현한 형태소 분석기를 사용하므로 위의 파이썬 라이브러리 외에 다음과 같은 개발자 도구와 라이브러리를 추가로 설치해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 명칭 | 기능 | 홈페이지 |\n",
    "|:-----|:-----|:---------|\n",
    "| Java Development Kit | 자바 컴파일러와 라이브러리 | <http://www.oracle.com/technetwork/java/javase/downloads/index.html> |\n",
    "| Microsoft Visual Visual C++ 2010 재배포 가능 패키지 | MSVC로 개발된 프로그램 실행 라이브러리 | <https://www.microsoft.com/ko-KR/download/details.aspx?id=14632> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치 순서\n",
    "위의 라이브러리들은 의존성이 있기 때문에 다음과 같은 순서로 설치하는 것이 안전하다.\n",
    "\n",
    "1. Java Development Kit\n",
    "1. jpype1\n",
    "1. Microsoft Visual Visual C++ 2010 재배포 가능 패키지\n",
    "1. konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자바 개발자 도구 설치\n",
    "이 강좌에서 모든 텍스트 처리는 파이썬 언어를 이용하여 이루어지지만 형태소 분석에는 자바 언어로 만들어진 도구를 사용한다. 그러므로 자바로 작성된 라이브러리를 원활히 사용할 수 있는 환경을 갖추어야 한다. 이를 위해 자바 개발 도구(Java Development Kit)를 설치한다.\n",
    "자바 개발 도구는 배포 사이트(<http://www.oracle.com/technetwork/java/javase/downloads/index.html>)에서 구할 수 있다. 표준판(SE: Standard Edition) 자바 개발자 도구 설치 프로그램을 내려받아 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![자바 개발 도구 표준판 내려받기](figs/jdk-download-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 자바 개발 도구 내려받기 그림을 누르면 운영체제와 플랫폼별로 준비된 설치 프로그램이 있는 곳으로 이동한다. 이곳에서 라이선스에 동의하고 자신의 환경에 맞는 설치 프로그램을 내려받아 실행하면 자바 개발 도구의 설치가 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![자바 개발 도구 설치 프로그램 내려받기](figs/jdk-download-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">konlpy 라이브러리는 JDK 6(1.6) 이상에서 동작하므로 시스템에 이미 JDK 설치된 경우 버전을 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPype 라이브러리 설치\n",
    "JPype(<http://jpype.readthedocs.io>)는 자바로 만들어진 라이브러리를 마치 파이썬으로 작성된 라이브러리처럼 사용할 수 있도록 해주는 라이브러리이다. 이 라이브러리는 C/C++ 확장 라이브러리로 윈도우에서 소스 패키지로 설치하려면 C/C++ 컴파일러가 필요하다. 하지만 이 과정은 매우 번거로우므로 특별한 경로를 통해 배포되는 바이너리 패키지를 설치하도록 하자.\n",
    "\n",
    "첫 번째 방법은 사용자 공동체에서 아나콘다 파이썬의 패키지 관리자인 conda를 위한 추가 패키지들을 제공하는 conda-forge(<https://conda-forge.org>)를 이용하는 것이다. 먼저 [메뉴]-[Anaconda3 (64bit)]-[Anaconda Prompt]를 실행하여 아나콘다 명령행 창을 열고 다음과 같이 하여 conda-forge 저장소를 추가하고 jpype1 라이브러리를 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> conda config --add channels conda-forge\n",
    "> conda install jpype1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> conda-forge 저장소의 추가는 한 번만 하면 된다. conda-forge에서 제공하는 패키지의 목록은 <https://conda-forge.org/feedstocks>에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 방법은 `whl` 형식의 바이너리 패키지를 비공식 윈도우 바이너리 파이썬 라이브러리 배포 사이트(Unofficial Windows Binaries for Python Extension Packages, <http://www.lfd.uci.edu/~gohlke/pythonlibs>)에서 내려 받아 pip으로 설치하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![비공식 윈도우 바이너리 파이썬 패키지 사이트의 JPype](figs/jpype-whl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 사용자들이 64비트 아키텍처 컴퓨터를 사용할 것이므로 `Jpype1-0.6.2-cp36-cp36m-win_amd64.whl` 파일을 내려받아 `다운로드` 디렉토리(`C:\\Users\\leekh\\Downloads`)에 저장한다. 이어서 [메뉴]-[Anaconda3 (64bit)]-[Anaconda Prompt]를 실행하여 아나콘다 명령행 창을 열고 다음과 같이 바이너리 패키지를 내려받은 디렉토리로 이동하여 pip 명령으로 패키지를 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> cd downloads\n",
    "> pip install Jpype1-0.6.2-cp36-cp36m-win_amd64.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">리눅스에서 맥오에스에서 JPype 라이브러리를 소스 패키지로 설치할 때에는 C/C++ 컴파일러와 JDK가 설치되 상태에서 `pip[3] install jpype1-py3` 명령으로 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSVC 런타임 설치\n",
    "마이크로소프트 비주얼 C/C++로 컴파일된 프로그램을 실행하려면 `msvc100.dll` 파일이 필요하다. 우리의 경우는 JPype 라이브러리가 그러한 경우이다. 보통 이 파일은 다른 경로로 설치되는 경우가 많지만 그렇지 않은 경우도 있다. 그러므로 konlpy 라이브러리에서 `msvcr100.dll` 파일이 없다는 메시지가 표시되거나 사전의 경로를 찾지 못한다는 오류가 발생하면 <https://www.microsoft.com/ko-KR/download/details.aspx?id=14632>에서 **Microsoft Visual C++ 2010 재배포 가능 패키지(x64)**를 내려받아 설치해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Microsoft Visual C++ 2010 재배포 가능 패키지](figs/msvcrt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy 라이브러리 설치\n",
    "이제 자바로 만들어진 형태소 분석 라이브러리를 JPype 라이브러리를 이용하여 파이썬에서 쉽게 쓸 수 있도록 해주는 라이브러리인 KoNLPy(<http://konlpy.org>)를 설치한다. 이 라이브러리 모듈은 아나콘다 명령행 창에서 pip으로 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소와 형태소 분석의 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소란 언어에 있어서 \"최소 의미 단위\"를 말한다. 이 때 의미는 어휘적 의미와 문법적 의미를 모두 포함한다. 형태소 분석이란 형태소보다 단위가 큰 언어 단위인 어절, 혹은 문장을 최소 의미 단위인 형태소로 분절하는 과정이다. 오해하지 말아야 할 것은 형태소**를** 분석하는 것이 아니라 형태소**로** 분석하는 것이라는 사실이다. \"어절 분석\" 혹은 \"형태론적 분석\"이 더 적합한 용어라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">그런 의미에서 영어 용어 morphological analysis가 더 적합하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석의 실제 예를 살펴보자. 다음은  \"나는 배가 아파서 걸어서 집에 갔습니다\"라는 문장을 형태소 분석한 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "나/대명사 는/보조사 배/명사+가/격조사 아프/형용사+아서/연결어미 걷/동사+어서/연결어미 \n",
    "집/명사+에/격조사 가/동사+았/선어말어미+습니다/어말어미\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 '나', '배', '아프', '걷', '집', '가'는 어휘적 의미를 나타냈고 있으며 이들을 어휘 형태소라고 부른다. 한편, '는', '가', '아서', '어서', '에', '았', '습니다'는 문법적 의미를 나타내며 이러한 형태소들을 문법 형태소라고 부른다.\n",
    "\n",
    "위의 형태소 분리 기준은 표준국어문법(남기심, 고영근 (2014) 『표준국어문법론』. 제4판. 탑출판사.)에 따른 것이다. 중세국어의 지식을 지닌 사람은 어말어미 '습니다'는 최소 의미 단위로의 분석이 덜 되었다고 주장할 수도 있다. 즉, '습'+'니'+'다'가 맞다고 할 수도 있다. 이러한 논의는 국어학에서의 논의이기에 우리는 현대어의 기술 문법인 표준국어문법을 따르기로 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석은 간단해 보이지만 사실은 매우 복잡한 과정이다. 많은 경우에 명사와 조사의 분리, 동사와 형용사의 어간과 어미의 분리는 각각의 이들 어휘의 사전만 있으면 가능할 것으로 보이지만 불규칙 용언의 활용 등을 처리하려면 복잡한 규칙이 필요하다. 또한 형태소 분석은 항상 정답을 결정하기 어려운 경우도 있다. 그러므로 성능이 좋은 형태소 분석기의 구현은 쉽지 않은 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">좋은 소식은 한국어 형태소 분석 기술이 성숙 단계에 이르러서 누구나 쓸 수 있는 공개 소프트웨어 형태소 분석기가 꽤 여러 개 존재한다는 것이다. 다만, 상업적으로 이용할 때에는 제한이 있을 수 있으니 형태소 분석기별로 사용자 라이선스를 잘 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "나는 -> 나/대명사+는/보조사 | 나/동사+는/관형형어미 | 날/동사+는/관형형어미\n",
    "걸어서 -> 걷/동사+어서/연결어미 | 걸/형용사+어서/연결어미 | 걸/동사+어서/연결어미\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에서 보는 것과 같이 형태소 분석의 기본 입력인 하나의 어절에 대하여 여러 개의 형태소 분석 결과가 있을 수 있기 때문이다. 그러므로 실용적인 형태소 분석기는 이와 같은 형태소 분석의 중의성을 해결하는 과정이 필요하다. 이 과정을 품사 태깅(part-of-speech tagging), 혹은 줄여서 태깅이라고 부른다. 정리하면 좁은 의미의 형태소 분석은 하나의 어절에 대하여 가능한 모든 형태소 분석 결과를 출력하는 절차이며, 넓은 의미의 형태소 분석은 문맥을 고려하여 하나의 어절에 대하여 가장 적합한 것으로 판단되는 하나의 형태소 분석 결과만을 출력하는 품사 태깅을 포함한 절차이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">사실 품사 태깅이라는 용어는 한국어의 형태소 분석에 있어서 아주 정확한 용어는 아니다. 품사는 그 정의상 단어에만 부여할 수 있는 것이다. 표준국어문법에서 조사는 단어이지만 어미는 단어가 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석 라이브러리 사용하기\n",
    "## 형태소 분석 라이브러리 동작 실험\n",
    "앞서 설치한 형태소 분석 라이브러리가 제대로 동작하는지 확인하기 위해 다음 스크립트를 실행해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "text = input(\"분석할 텍스트를 입력하세요: \")\n",
    "ma_result = komoran.pos(text)\n",
    "\n",
    "for lex, pos in ma_result:\n",
    "    print(\"{}\\t{}\".format(lex, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komoran에서 지원하는 형태소 분석 메소드 몇 가지를 더 실험해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"어느새 뜨거운 여름이 되어 빙수가 생각난다.\"\n",
    "print(komoran.morphs(text))\n",
    "print(komoran.nouns(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과에서 볼 수 있는 것처럼 `morphs()` 메소드는 분절된 형태소들을, `nouns()` 메소드는 분절된 형태소들 가운데 명사만을 골라서 돌려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy는 현재 다섯 종류의 형태소 분석 라이브러리를 지원한다. 다음은 KoNLPy에서 지원하는 형태소 분석 라이브러들의 정보 요약이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 클래스명| 분석기명 | 구현 언어 | 홈페이지 | 라이센스 |\n",
    "|:--------|:---------|:---------:|:---------|:---------|\n",
    "| Kkma    | 꼬꼬마   | 자바      | http://kkma.snu.ac.kr | GPL 2.0 |\n",
    "| Komoran | Komoran  | 자바      | https://github.com/shineware/komoran-2.0 | Apache License 2.0 |\n",
    "| Hannanum | 한나눔  | 자바      | http://semanticweb.kaist.ac.kr/hannanum | GPL 3.0 |\n",
    "| Twitter | twitter-korean-text  | 스칼라    | https://github.com/twitter/twitter-korean-text | Apache License 2.0 |\n",
    "| Mecab   | 은전한닢  | C++       | https://bitbucket.org/eunjeon/mecab-ko  | GPL 2.0, LGPL 2.1, BSD |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 가운데 윈도우에서 이용할 수 있는 라이브러리들은 자바로 구현된 Kkma, Komoran, Hannanum와 스칼라로 구현된 Twitter이다. C++로 구현된 Mecab은 윈도우에서 지원하지 않는다.\n",
    "\n",
    "이들 라이브러리들은 실행 속도, 정확도, 사용자 사전 용이성 등의 특성이 모두 다르다. 다음은 KoNLPy 설명서(<http://konlpy.org/en/v0.4.4/morph/#comparison-between-pos-tagging-classes>)에서 가져온 비교 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![KoNLPy 형태소 분석 클래스의 속도 비교](figs/konlpy-time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전반적인 실행 속도는 당연하게도 C/C++로 구현된 Mecab이 가장 빠르다. 그러나 현재 Mecab은 맥오에스와 리눅스에서만 지원된다. \n",
    "* 그 다음으로는 스칼라로 구현된 Twitter와 자바로 구현된 Hannanum, Kkma가 뒤를 따르고 있다.\n",
    "* 현재 가장 느린 것은 Komoran이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">최근 소식(<https://groups.google.com/forum/?hl=ko#!topic/eunjeon/Dzohqj4n3QI>)에 따르면 Mecab의 윈도우용 바이너리가 지원되기 시작하였기에 KoNLPy에서도 곧 윈도우에서 Mecab을 지원할 수 있을 것으로 기대한다. Komoran의 경우에는 3.0(<http://www.shineware.co.kr/products/komoran>)에서 큰 속도 향상이 있는 것으로 알려졌다. Twitter는 프로젝트의 이름이 바뀌어 계속 개발되고 있다(<https://github.com/open-korean-text/open-korean-text>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 형태소 분석기들은 처리 속도뿐만 아니라 문장 분절, 자동 띄어쓰기의 지원, 미등록어 처리, 사용자 사전 지원 등에 모든 다른 특성을 가지고 있다. 이 강좌에서는 분석 속도는 느리지만 체언과 용언 모두에서 일정 수준 이상의 분석 정확도를 보이며, 지속적으로 개발되고 있는 Komoran을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">사용자 사전의 지원과 분석 속도에 있어서는 Hannanum이 유리하지만 실험 결과 용언에 대한 분석 성능이 다른 형태소 분석기들에 비해 떨어진다. Twitter는 분석 속도는 빠르지만 품사 분류가 거칠다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사 집합\n",
    "`pos()` 메소드가 돌려주는 형태소 분석 결과는 형태소와 품사로 이루어진 튜플의 리스트이다. 이 형태소 분석 결과를 이용하기 위해서는 형태소 분석기가 돌려주는 품사 집합(part-of-speech tag set)를 알고 있어야 한다. 이 강좌에서 우리가 사용할 `Komoran` 클래스의 품사 집합은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">이 품사 집합은 Komoran에 고유한 것이 아니라 국가 주도로 진행된 21세기 세종 프로젝트(<https://ithub.korean.go.kr>)를 통해 제안된 표준 품사 집합이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 태그 | 품사 | 소분류 | 대분류 |\n",
    "|:-----|:-----|:-------|:-------|\n",
    "| NNG  | 일반 명사 | 명사 | 체언 |\n",
    "| NNP  | 고유 명사 | 명사 | 체언 |\n",
    "| NNB  | 의존 명사 | 명사 | 체언 |\n",
    "| NR   | 수사 | 수사 | 체언 |\n",
    "| NP   | 대명사 | 대명사 | 체언 |\n",
    "| VV   | 동사 | 동사 | 용언 |\n",
    "| VA   | 형용사 | 형용사 | 용언 |\n",
    "| VX   | 보조 용언 | 보조용언 | 용언 |\n",
    "| VCP  | 긍정 지정사 | 지정사 | 용언 |\n",
    "| VCN  | 부정 지정사 | 지정사 | 용언 | \n",
    "| MM   | 관형사 | 관형사 | 수식언 |\n",
    "| MAG  | 일반 부사 | 부사 | 수식언 |\n",
    "| MAJ  | 접속 부사 | 부사 | 수식언 |\n",
    "| IC   | 감탄사 | 감탄사 | 독립언 |\n",
    "| JKS  | 주격 조사 | 조사 | 관계언 |\n",
    "| JKC  | 보격 조사 | 조사 | 관계언 |\n",
    "| JKG  | 관형격 조사 | 조사 | 관계언 |\n",
    "| JKO  | 목적격 조사 | 조사 | 관계언 |\n",
    "| JKB  | 부사격 조사 | 조사 | 관계언 |\n",
    "| JKV  | 호격 조사 | 조사 | 관계언 |\n",
    "| JKQ  | 인용격 조사 | 조사 | 관계언 |\n",
    "| JC   | 접속 조사 | 조사 | 관계언 |\n",
    "| JX   | 보조사 | 조사 | 관계언 |\n",
    "| EP   | 선어말어미 | 어미 | 관계언 |\n",
    "| EF   | 종결 어미 | 어미 | 관계언 |\n",
    "| EC   | 연결 어미 | 어미 | 관계언 |\n",
    "| ETN  | 명사형 전성 어미 | 어미 | 관계언 |\n",
    "| ETM  | 관형형 전성 어미 | 어미 | 관계언 |\n",
    "| XPN  | 체언 접두사 | 접두사 | 접사 |\n",
    "| XSN  | 명사파생 접미사 | 접미사 | 접사 |\n",
    "| XSV  | 동사 파생 접미사 | 접미사 | 접사 |\n",
    "| XSA  | 형용사 파생 접미사 | 접미사 | 접사 |\n",
    "| XR   | 어근 | 어근 | 어근 |\n",
    "| SF   | 마침표, 물음표, 느낌표 | 기호 | 기호 |\n",
    "| SE   | 줄임표 | 기호 | 기호 |\n",
    "| SS   | 따옴표,괄호표,줄표 | 기호 | 기호 |\n",
    "| SP   | 쉼표, 가운뎃점, 콜론, 빗금 | 기호 | 기호 |\n",
    "| SO   | 붙임표(물결, 숨김, 빠짐) | 기호 | 기호 |\n",
    "| SW   | 기타기호 (논리수학기호, 화폐기호)  | 기호 | 기호 |\n",
    "| SH   | 한자 | 한자 | 기호 |\n",
    "| SL   | 외국어 | 외국어 | 기호 |\n",
    "| SN   | 숫자 | 숫가 | 기호 |\n",
    "| NF   | 명사추정범주 | | |\n",
    "| NV   | 용언추정범주 | | |\n",
    "| NA   | 분석불능범주 | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 품사 분류는 학교 문법의 품사 분류와 대체로 일치하는데 가장 다른 부분은 '이다'와 '아니다'를 각각 긍정지정사와 부정지정사인 용언으로 분류한 것이다. 위의 품사들 가운데 텍스트 분석에서 일반적으로 텍스트 마이닝에서 다루는 품사들은 어휘적 의미를 지니면서 개념어인 MNG(일반 명사), NNP(고유 명사), VV(동사), VA(형용사)이며, 경우에 따라 MM(관형사), MAG(일반 부사), MAJ(접속 부사)도 다룬다. 그러나 텍스트 분석의 목표와 분석 대상 텍스트의 성격에 따라 다른 품사들도 얼마든지 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석의 입출력 설계\n",
    "이미 밝힌 대로 이 강좌에서는 분석 대상 자료를 JSON 라인 형식으로 저장한다. 그러므로 형태소 분석 대상인 문자열, 예를 들어 신문 기사 본문은 JSON의 특정 키에 대당하는 값으로 주어질 것이다. 또한 이 강좌의 텍스트 처리는 오프라인 일괄 처리를 전제하므로 형태소 분석을 일괄적으로 수행하여 저장한다. 따라서  형태소 분석 결과 역시 JSON 형식으로 저장되어야 할 것이다.\n",
    "\n",
    "### 입력\n",
    "`pos()` 메소드의 입력으로는 제법 긴 문자열을 지정할 수 있다. 그러나 길이가 제법 길어질 수 있는 신문 기사 본문 한 건을 모두 `pos()` 메소드의 인자로 주는 것은 효과적이지 못하다. 따라서 입력 텍스트의 적절한 분절이 필요하다. 여러모로 가장 적절한 분절은 문장이다. 형태소 분석기 가운데에는 형태소 분석 전처리, 혹은 후처리 단계로 문장 분절을 지원하는 것들이 있다. 그러나 모든 형태소 분석기가 해당 기능을 가진 것은 아니다. 우리가 사용하는 Komoran 역시 문장 분절을 지원하지 않으므로 외부에서 문장 분절을 수행해야 한다.\n",
    "\n",
    "간단한 문장 분절을 위해 지난 강의에서 언급한 대로 문장의 종결을 나타내는 문장 부호인 '.', '?', '!'와 공백 문자의 연속을 이용한다. 이때 효율적인 패턴 일치를 위해 정규 표현을 이용한다. 다음은 이를 코드로 구현한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 정규 표현을 이용한 문장 분절 함수(split() 함수 사용)\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = [sentence for sentence in re.split(\"(?<=[.?!]) \", text.strip())]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = \"파이썬은 데이터 분석에 적절한 언어이다. \" + \\\n",
    "    \"하지만 프로그램의 실행 속도가 빠르지 않다(?)는 평가가 있지 않은가! \" + \\\n",
    "    \"실행 속도는 언어 선택에 있어서 중요한 요소이긴 하지만 항상 그런가?\\n\\n\\n\"\n",
    "    \n",
    "sentences = split_sentences(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence + \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 사용된 정규 표현은 생김새가 복잡한데, `split()` 함수의 의미는 '.', '?', '!' 가운데 한 문자가 선행하는 공백 문자를 기준으로 입력 문자열을 분절하라는 뜻이다.\n",
    "\n",
    "### 출력\n",
    "형태소 분석 결과를 응용 프로그램에서 바로 사용할 경우에는 리스트로 주어지는 형태소 분석 결과를 그대로 쓰면 된다. 그런데 형태소 분석 결과를 파일에 저장하였다가 재사용하려는 경우에는 몇 가지 고려해야 할 사항이 있다.\n",
    "\n",
    "첫 번째는 입력 텍스트의 구조, 즉 단락 구분이나 문장 구분, 그리고 어절 구분 등을 얼마나 반영할 것인가의 고려이다. 이는 텍스트 분석의 목표와 과제에 따라 결정된다. 이 강좌에서는 단락의 구분은 하지 않으며, 문장 구분은 유지하기로 하자. 어절의 구분은 그 방법과 형태를 알아 보기는 하되 실제로 반영하지는 않는다.\n",
    "\n",
    "두 번째는 저장 형식의 선택이다. 형태소 분석 결과는 앞에서 논의한 것처럼 일정 수준의 구조적 정보를 담고 있으므로 재사용의 편의를 위하여 구조가 반영된 형태로 저장하는 것이 좋다. 이 강좌에서는 JSON 형식을 이용한다. 예를 들면 아래과 같이 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    text = \"어느새 가을이 성큼 다가왔습니다.\",\n",
    "    ma_res = [\n",
    "        [\n",
    "            [\"어느새\", \"MAG\"],\n",
    "            [\"가을\", \"NNG\"],\n",
    "            [\"이\", \"JKS\"],\n",
    "            [\"성큼\", \"MAG\"],\n",
    "            [\"다가오\", \"VV\"],\n",
    "            [\"았\", \"EP\"],\n",
    "            [\"습니다\", \"EF\"],\n",
    "            [\".\", \"SF\"]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 형태소 분석 결과를 저장하는 리스트 `ma_res` 안에 리스트가 한 겹 더 있는 것은 입력 텍스트가 여러 개의 문장으로 분절될 경우 문장별 형태소 분석 결과를 분리하여 저장하기 위해서이다. \n",
    "즉, `ma_res`는 입력 텍스트의 문장별 형태소 분석 결과를 원소로 하는 리스트이다.\n",
    "\n",
    "앞서 언급한 바와 같이 경우에 따라서는 개별 형태소와 문장 사이에 존재하는 어절 층위를 반영하는 것이 필요할 수도 있다. 그렇게 하면 아래와 같이 리스트의 층위가 한 겹 더 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    text = \"어느새 가을이 성큼 다가왔습니다.\",\n",
    "    ma_res = [\n",
    "        [\n",
    "            [\n",
    "                [\"어느새\", \"MAG\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"가을\", \"NNG\"],\n",
    "                [\"이\", \"JKS\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"성큼\", \"MAG\"],\n",
    "            ],\n",
    "            [\n",
    "                [\"다가오\", \"VV\"],\n",
    "                [\"았\", \"EP\"],\n",
    "                [\"습니다\", \"EF\"],\n",
    "                [\".\", \"SF\"]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 단락, 장절 등 문장보다 단위가 큰 언어 단위 구분도 필요에 따라 도입할 수 있을 것이다. 이 강좌에서는 문장과 형태소의 구분만 유지하는 JSON 형식을 사용한다.\n",
    "\n",
    "### 참고\n",
    "전산 언어학이나 말뭉치 언어학에서는 다음과 같은 어절 단위 수직 텍스트 형식이나 문장 단위 수평 형식도 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 어절 단위 수직 텍스트 형식\n",
    "어느새 어느새/MAG\n",
    "가을이 가을/NNG+이/JKS\n",
    "성큼  성금/MAG\n",
    "다가왔습니다. 다가오/VV+았/EP+습니다/EF+./SF\n",
    "\n",
    "# 문장 단위 수평 형식\n",
    "어느새_어느새/MAG 가을이_가을/NNG+이/JKS 성큼_성큼/MAG 다가왔습니다._다가오/VV+았/EP+습니다/EF+./SF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 XML에 기반한 주석 말뭉치 형식의 국제 표준인 XCES(<http://www.xces.org>)도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최근 R 기반 텍스트 마이닝에서 Tidy Text라는 수직 텍스트 형식이 제안되어 인기를 얻고 있다. 원본 텍스트의 구조를 최대한 보존하는 형식으로 의미가 있다. 관련 페이지(<http://tidytextmining.com/tidytext.html>)를 참조하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석 함수의 구현\n",
    "앞서 구현한 문장 분절 함수 `split_sentences()`와 형태소 분석 클래스의 `pos()` 메소드를 결합하여 문장 단위의 형태소 분석 결과를 돌려주는 함수를 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 형태소 분석 함수\n",
    "import re\n",
    "import ujson\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    all_sentences = []\n",
    "    lines = [line for line in text.strip().splitlines() if line.strip]\n",
    "    \n",
    "    for line in lines:\n",
    "        sentences = re.split(\"(?<=[.?!]) \", line)\n",
    "        all_sentences += sentences\n",
    "    \n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "def get_morph_anal(analyzer, text):\n",
    "    sent_morph_anals = []\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_morph_anal = analyzer.pos(sentence)\n",
    "        sent_morph_anals.append(sent_morph_anal)\n",
    "        \n",
    "    return sent_morph_anals\n",
    "\n",
    "\n",
    "def main():\n",
    "    my_text = \"\"\"<벌레이야기>\n",
    "\n",
    "1\n",
    "아내는 알암이의 돌연스런 가출이 유괴에 의한 실종으로 확실시되고 난 다음에도 한동안은 악착스럽게 자신을 잘 견뎌 나가고 있었다. 그것은 아이가 어쩌면 행여 무사히 되돌아오게 될지도 모른다는 간절한 희망과, 녀석에게 마지막 불행한 일이 생기기 전에 어떻게든지 놈을 다시 찾아내고 말겠다는 어미로서의 강인한 의지와 기원 때문인 것 같았다.\n",
    "지난해 5월 초. 어느 날 알암이가 학교에서 돌아올 시각이 훨씬 지나도록 귀가를 안 했다.\n",
    "\"\"\"\n",
    "\n",
    "    komoran = Komoran()\n",
    "    ma_res = get_morph_anal(komoran, my_text)\n",
    "    print(ma_res)\n",
    "    \n",
    "    \n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 `split_sentence()` 함수는 기본적으로 앞서 실험한 것과 같지만 입력 텍스트에 존재하는 기본적인 줄 구분을 유지하도록 하였다. 이 때 한 문장이 여러 줄로 나뉘는 경우는 없다고 전제하고 한 줄 안에 여러 개의 문장이 있을 수 있는 것으로 간주하고 문장 분절을 수행한다.  이 때 줄바꿈 문자가 두 번 이어져서 분절되는 빈 문장은 제거한다.\n",
    "\n",
    "`get_morph_anal()` 함수는 형태소 분석기 객체와 분석 대상 텍스트를 인자로 받아 문장 문절을 수행한 뒤 형태소 분석 결과를 문장별로 수행하여 돌려준다. 형태소 분석기 객체를 함수의 바깥에서 초기화하여 인자로 받는 이유는 첫째로 형태소 분석 객체를 초기화하는데 드는 비용을 감안하여 되도록 한 번 초기화한 형태소 분석 객체를 반복적으로 사용하기 위함이며, 둘째로 KoNLPy에서 제공하는 다른 형태소 분석도 이용할 수 있는 가능성을 열어두기 위함이다. 돌려진 형태소 분석 결과를 JSON 등의 형태로 저장하는 것은 위의 함수를 호출하는 코드에서 하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석 후처리와 사용자 사전의 작성\n",
    "## 파생어 분석의 후처리\n",
    "Komoran 형태소 분석기는 `강렬하고 섹시한`이라는 텍스트를 다음과 같이 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[('강렬', 'XR),\n",
    " ('하', 'XSA'),\n",
    " ('고', 'EC'),\n",
    " ('섹시', 'XR),\n",
    " ('하', 'XSA'),\n",
    " ('ㄴ', 'ETM')] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 형용사인 `강렬하다`와 `섹시하다`를 파생 단계까지 분석한다. 텍스트 분석의 목적에 따라 이러한 세부 분석이 적합할 때도 있지만 경우에 따라서는 다음의 분석이 더 적합할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[('강렬하', 'VA),\n",
    " ('고', 'EC'),\n",
    " ('섹시한', 'VA),\n",
    " ('ㄴ', 'ETM')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러므로 경우에 따라서는 형태소 분석 결과에서 파생어 분석 처리를 어휘 단계로 후처리를 해야 한다. 동사 파생 접미사(XSV), 체언 접두사(XPN), 명사 파생 접미사(XSN) 경우에도 비슷한 처리가 필요할 수 있다. 어떠한 처리를 해야 할지는 대규모 말뭉치에 대한 형태소 분석 결과를 면밀히 검토하고 결정해야 한다.\n",
    "\n",
    "형태소 분석 결과의 후처리에는 부분 리스트를 치환하는 방법을 적용할 수 있다. 다음 소스를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 부분 리스트 치환에 의한 형태소 분석 결과 후처리\n",
    "import itertools\n",
    "\n",
    "\n",
    "def find_sublists(seq, sublist):\n",
    "    length = len(sublist)\n",
    "    for index, value in enumerate(seq):\n",
    "        if value == sublist[0] and seq[index:index+length] == sublist:\n",
    "            yield index, index+length\n",
    "            \n",
    "\n",
    "def replace_sublist(seq, target, replacement, maxreplace=None):\n",
    "    sublists = find_sublists(seq, target)\n",
    "    if maxreplace:\n",
    "        sublists = itertools.islice(sublists, maxreplace)\n",
    "    for start, end in sublists:\n",
    "        seq[start:end] = replacement\n",
    "        \n",
    "        \n",
    "post_proc_pairs = [\n",
    "    ([(\"강렬\", \"XR\"), (\"하\", \"XSA\")], [(\"강렬하\", \"VA\")]),\n",
    "    ([(\"섹시\", \"XR\"), (\"하\", \"XSA\")], [(\"섹시하\", \"VA\")])\n",
    "]\n",
    "        \n",
    "ma_res = [(\"강렬\", \"XR\"), (\"하\", \"XSA\"), (\"고\", \"EC\"), \n",
    "          (\"섹시\", \"XR\"), (\"하\", \"XSA\"), (\"ㄴ\", \"ETM\")]\n",
    "\n",
    "for src, dst in post_proc_pairs:\n",
    "    replace_sublist(ma_res, src, dst)\n",
    "    \n",
    "print(ma_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 실행해 보면 파생어에 대한 후처리가 의도한 대로 동작하는 것을 확인할 수 있다. 위의 코드의 핵심은 이른바 생성자(generator)의 사용이다. 이는 입력으로 주어진 형태소 분석 결과 리스트를 지속적으로 업데이트하면서 치환을 진행하는 데에 결정적인 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 코드는 스택오버플로우에서 질문과 답변(<http://stackoverflow.com/questions/12898023/replacing-a-sublist-with-another-sublist-in-python>)으로 주어진 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 정규 표현을 이용하여 단일 패스에 의해 다중 문자열의 치환을 수행하는 방법도 있다. 다음 소스는 딕셔너리의 키와 값으로 주어지는 다중 문자열 치환을 수행하는 방법을 담고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현을 이용한 형태소 분석 결과의 후처리\n",
    "# https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch01s19.html\n",
    "import re\n",
    "\n",
    "\n",
    "PATCH_PATS = {\n",
    "    \"강렬/XR 하/XSA\": \"강렬하/VA\",\n",
    "    \"섹시/XR 하/XSA\": \"섹시하/VA\"\n",
    "}\n",
    "ESC_SRC_PATS = [re.escape(src_pat) for src_pat in PATCH_PATS]\n",
    "SRC_PAT_RE = re.compile(\"|\".join(ESC_SRC_PATS))\n",
    "\n",
    "\n",
    "def repl(match):\n",
    "    \"\"\"주어진 match 객체에 대한 치환 패턴을 돌려준다.\"\"\"\n",
    "    \n",
    "    pat = PATCH_PATS[match.group(0)]\n",
    "    \n",
    "    return pat\n",
    "\n",
    "\n",
    "def multiple_replace(text):\n",
    "    \"\"\"주어진 문자열에 대한 다중 문자열 치환을 수행한 결과를 돌려준다.\"\"\"\n",
    "    \n",
    "    patched_text = SRC_PAT_RE.sub(repl, text)\n",
    "    \n",
    "    return patched_text\n",
    "\n",
    "\n",
    "ma_res = \"강렬/XR 하/XSA 고/EC 섹시/XR 하/XSA ㄴ/ETM\"\n",
    "patch_pats = {\n",
    "    \"강렬/XR 하/XSA\": \"강렬하/VA\",\n",
    "    \"섹시/XR 하/XSA\": \"섹시하/VA\"\n",
    "}\n",
    "patched_ma_res = multiple_replace(ma_res)\n",
    "print(patched_ma_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 실행해 보면 파생어에 대한 후처리가 의도한 대로 동작하는 것을 확인할 수 있다. 위의 코드의 핵심은 정규 표현 객체의 `sub()` 메소드의 첫 번째 인자로 매치 객체를 인자로 받아 적절한 처리를 하여 치환 목표 문자열을 생성하는 함수를 지정할 수 있음을 이용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표형의 처리\n",
    "형태소 분석기는 어휘 형태소의 대표형을 찾아준다. 그러나 조사나 어미에 대해서는 그러한 처리를 하지 않는다. 즉, '학교를', '신발을'에 대하여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "학교/NNG+를/JKO\n",
    "신발/NNG+을/JKO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로 분석한다. 이 때 '를'과 '을'은 의미와 기능이 같은 목적격 조사로서 앞에 오는 명사류의 말음이 자음인지 모음인지에 따라 선택되는 음운론적 변이형이다. 그러므로 경우에 따라 이들 가운데 하나를 대표형으로 설정하고 다른 변이형을 대표형으로 치환해 주는 후처리가 요청될 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오분석의 교정\n",
    "위에서 보인 파생어 후처리 방법을 오분석의 교정에도 사용 가능하다. 특히 미등록 형태소로 인해 발생하는 오분석의 처리는 실용적으로 매우 중요하다. 물론 미등록 형태소의 분석을 위해서는 사용자 사전을 작성하는 것이 정석일 것이다. 그러나 현실적으로는 선제적으로 모든 형태소를 갖춘 사용자 사전을 작성하는 것이 불가능하다. 따라서 분석 대상 핵심 어휘에 대한 형태소 분석 결과에 대한 후처리로 문제를 해결하는 것이 바람직하다. 그러려면 초벌 형태소 분석 결과로부터 오분석 패턴을 수집하여 분석하는 절차를 거쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 작성\n",
    "형태소 분석기에서는 형태소 분석을 위해 형태소 사전을 이용한다. 형태소 사전에는 개별 형태소와 이들의 품사, 불규칙 정보, 형태소 간의 연결 가능성을 나타내는 접속 정보 등이 포함된다. 이 사전에 모든 형태소가 포함될 수는 없으므로 형태소 분석기들을 사용자 형태소 사전을 통해 어휘를 보충할 수 있도록 하는 경우가 많다. 또한 어절별로 형태소 분석 결과를 미리 지정하여 사용하는 기분석 사전을 지원하기도 한다.\n",
    "\n",
    "이 강좌에서 사용하는 Komoran 형태소 분석기는 원래 사용자 형태소 사전을 지원하는데 안타깝게도 KoNLPy에서는 이 기능을 사용할 수 없다. Komoran보다는 형태소 분석 정확도가 떨어지는 Hannanum과 Kkma 형태소 분석기에서는 사용자 형태소 사전을 지원한다. 그런데 Kkma 형태소 분석기에서 사용자 사전을 조성하여 사용하려면 자바 아카이브(`jar`) 파일의 패키징을 풀었다가 다시 패키징하는 등의 매우 복잡한 과정을 거쳐야 하므로 현실적으로는 Hannanum에서만 사용자 사전을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Komoran 형태소 분석기의 저자가 작성한 파이썬 모듈을 이용할 수도 있을 것이다. 이 모듈은 깃허브(<https://github.com/shineware/komoranPy_2.0>)에서 내려받을 수 있는데 충분한 실험이 되지 않은 듯하다. 최근 정보화진흥원에서 Hannanum 형태소 분석기에서 사용할 수 있는 확장된 형태소 사전을 공개하였다. 배포 페이지(<https://kbig.kr/index.php?page=0&sv=title&sw=&q=knowledge/pds_&tgt=view&page=1&idx=16451&sw=&sv=title?q=node/16451>)에서 내려받을 수 있다. 제공되는 사전의 규모가 매우 크므로 유의해야 한다. 물론 이 사전을 약간의 가공을 거쳐 사용자 사전을 지원하는 다른 형태소 분석기에서 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습\n",
    "1. TSV 형식의 뉴스 기사 파일을 형태소 분석하여 JSON 라인 형식의 텍스트 파일을 작성하라.\n",
    "1. TSV 형식의 메타 정보 파일과 곡별 가사 텍스트 파일로 이루어진 KPOP 데이터 파일을 형태소 분석하여 JSON 라인 형식의 텍스트 파일을 생성하라."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트마이닝캠프",
   "courseyear": "2017",
   "date": "2017.08.10",
   "title": "형태소 분석과 후처리"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
